<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"> 
  </head>
  <body>
    <div id="renkon">
      <link rel="stylesheet" href="../bridge/session.css" />
      <script type="reactive">
        const localMediaModule = import("../bridge/localmedia.js");
        const {audioBufferToWav} = import("../bridge/wav.js");
        const {toBase64} = import("../bridge/toBase64.js");

        const {html, render} = import('../preact.standalone.module.js');
        const setSessionButton = Events.listener(document.querySelector("#setSessionButton"), "click", (evt) => evt);
        const audioContext = Behaviors.keep(Events.listener(document.querySelector("#logo"), "click", (evt) => new window.AudioContext()));

        console.log("audioContext", audioContext);
        const sessionSpec = (() => document.querySelector("#sessionName").textContent)(setSessionButton);

        const localMedia = new localMediaModule.LocalMedia({
          videoSource: false,
          onstreamchange: (stream) => {
          }
        });

        const streams = localMedia.setup();

        const source = ((audioContext, localMedia, _streams) => {
          console.log("in source", audioContext, localMedia);
          return new window.MediaStreamAudioSourceNode(audioContext, {mediaStream: localMedia.stream})
        })(audioContext, localMedia, streams);

        const processor = ((audioContext) => {
            return audioContext.audioWorklet.addModule(`../bridge/audio-visualizer.js`).then(() => {
                return new window.AudioWorkletNode(audioContext, "processor");
            });
        })(audioContext);

        console.log(processor);

        const inputs = Events.observe((notifier) => {
            processor.port.onmessage = (event) => {
                notifier(event.data);
            }
            source.connect(processor);
            return () => source.disconnect(processor);
        }, {queued: true});

        const voiceChunk = Events.receiver();

        console.log("voiceChunk", voiceChunk);

        // {label: number, data: Array<[Array<number>, Array<number>]>}
        const longerBuffer = Behaviors.collect({label: 1, data: []}, inputs, (old, inputs) => {
            const newData = inputs.map(obj => obj.input);
            if (old.data.length < 640) {
                old.data.push(...newData); // could be a bad idea
                return old;
            } else {
                const newId = old.label + 1;
                Events.send(voiceChunk, old);
                return {label: newId, data: [...newData]};
            }
        });

        const wav = ((voiceChunk) => {
            const zip = (pairs) => {
                const length = pairs[0][0].length * pairs.length;
                const a = new Float32Array(length);
                const b = new Float32Array(length);
                let index = 0;
                for (let i = 0; i < pairs.length; i++) {
                    a.set(pairs[i][0], index);
                    b.set(pairs[i][1], index);
                    index += pairs[i][0].length;
                }
                return [a, b];
            }
            return {label: voiceChunk.label, wav: audioBufferToWav(44100, zip(voiceChunk.data))};
        })(voiceChunk);
        console.log("wav", wav);
    
        const response = ((wav) => {
            if (wav.label === 2) {
                return fetch("https://substrate.home.arpa/faster-whisper/v1/transcribe", {
                    method: "POST",
                    mode: "cors",
                    headers: {
                        "Content-Type": "application/json",                      
                    },
                    body: JSON.stringify({audio_data: toBase64(new Uint8Array(wav.wav)), audio_metadata: {mime_type: "audio/wav"}, task: "transcribe"})
                });
                // const {ReflectCommands} = import("https://substrate.home.arpa/tool-call/js/commands.js");
                // const r = new ReflectCommands("https://substrate.home.arpa").reflect();
                // return r["faster-whisper:transcribe-data"].run({audio_data: toBase64(new Uint8Array(wav.wav)), audio_metadata: {mime_type: "audio/wav"}, task: 'transcribe'})
            }
        })(wav);

        const responseJson = response.json();
        console.log(responseJson);

      </script>
      <canvas id="oscilloscope" width=320 height=320></canvas>
      <div class="flex flex-wrap align-center px-6 py-4">
        <h1 id="logo" class="py-1 text-xl font-bold grow">bridge</h1>
      </div>
      <div id="sessionNameHolder">
        <div contentEditable id="sessionName">https://substrate.home.arpa/bridge;sessions=sp-01J343NFDMW9B5XG6S6XJZDNE8;id=cqcqi10ri6qs739svkk1</div>
        <button id="setSessionButton">Set</button>
      </div>
      <div id="sessions"></div>
    </div>

    </div>
    <script type="module">
      import("./renkon.js").then((mod) => mod.view());
    </script>
  </body>
</html>
