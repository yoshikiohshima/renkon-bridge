<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"> 
  </head>
  <body>
    <div id="renkon">
      <link rel="stylesheet" href="../bridge/session.css">
      <script type="reactive">
        const localMediaModule = import("../bridge/localmedia.js");
        const {audioBufferToWav} = import("../bridge/wav.js");
        const {toBase64} = import("../bridge/toBase64.js");

        const {html, render} = import('../preact.standalone.module.js');
        const audioContext = Behaviors.keep(Events.listener(document.querySelector("#logo"), "click", (evt) => new window.AudioContext()));

        const localMedia = new localMediaModule.LocalMedia({
          videoSource: false,
          onstreamchange: (stream) => {
          }
        });

        const streams = localMedia.setup();

        const source = ((audioContext, localMedia, _streams) => {
          console.log("in source", audioContext, localMedia);
          return new window.MediaStreamAudioSourceNode(audioContext, {mediaStream: localMedia.stream})
        })(audioContext, localMedia, streams);

        const processor = ((audioContext) => {
            return audioContext.audioWorklet.addModule(`../bridge/audio-visualizer.js`).then(() => {
                return new window.AudioWorkletNode(audioContext, "processor");
            });
        })(audioContext);

        console.log(processor);

        const inputs = Events.observe((notifier) => {
            processor.port.onmessage = (event) => {
                notifier(event.data);
            }
            source.connect(processor);
            return () => source.disconnect(processor);
        }, {queued: true});

        const voiceChunk = Events.receiver();

        console.log("voiceChunk", voiceChunk);

        // {label: number, data: Array<[Array<number>, Array<number>]>}
        const longerBuffer = Behaviors.collect({label: 1, data: []}, inputs, (old, inputs) => {
            const newData = inputs.map(obj => obj.input);
            if (old.data.length < 1280) {
                old.data.push(...newData); // could be a bad idea
                return old;
            } else {
                const newId = old.label + 1;
                Events.send(voiceChunk, old);
                return {label: newId, data: [...newData]};
            }
        });

        const wav = ((voiceChunk) => {
            const zip = (pairs) => {
                const length = pairs[0][0].length * pairs.length;
                const a = new Float32Array(length);
                const b = new Float32Array(length);
                let index = 0;
                for (let i = 0; i < pairs.length; i++) {
                    a.set(pairs[i][0], index);
                    b.set(pairs[i][1], index);
                    index += pairs[i][0].length;
                }
                return [a, b];
            }
            return {label: voiceChunk.label, wav: audioBufferToWav(44100, zip(voiceChunk.data))};
        })(voiceChunk);
        console.log("wav", wav);
    
        const response = ((wav) => {
                return {
                    label: wav.label,
                    response: fetch("https://substrate.home.arpa/faster-whisper/v1/transcribe", {
                        method: "POST",
                        mode: "cors",
                        headers: {
                            "Content-Type": "application/json",                      
                        },
                        body: JSON.stringify({audio_data: toBase64(new Uint8Array(wav.wav)), audio_metadata: {mime_type: "audio/wav"}, task: "transcribe"})
                    })
                }
                // const {ReflectCommands} = import("https://substrate.home.arpa/tool-call/js/commands.js");
                // const r = new ReflectCommands("https://substrate.home.arpa").reflect();
                // return r["faster-whisper:transcribe-data"].run({audio_data: toBase64(new Uint8Array(wav.wav)), audio_metadata: {mime_type: "audio/wav"}, task: 'transcribe'})
        })(wav);

        const waited = Events.resolvePart(response.response, response);
        const responseJson = ((waited) => {
            return {label: waited.label, json: waited.response.json()};
        })(waited);
        const fullResponse = Events.resolvePart(responseJson.json, responseJson);
        const transcriptions = Behaviors.collect([], fullResponse, (old, now) => {
            function find(array, label) {
                for (let i = 0; i < array.length - 1; i++) {
                    if (array[i].labe === label - 1) {
                        return i;
                    }
                }
                return array.length;
            }
            const index = find(old, now.label);
            old[index] = now;
            return [...old];
        });
        const results = transcriptions.map((transcript) => {
            return html`<div id="transcript-${transcript.label}">${transcript.json.segments.map((seg) => seg.text)}</div>`;
        })
        render(results, document.querySelector("#entries"));
      </script>
      <div class="flex flex-col grow">
        <button id="logo" class="py-1 text-xl font-bold grow">Start</button>
        <div id="mic-controls" class="flex space-x-2">
          <div id="mic-container"></div>
          <button name="mic-mute" id="mic-mute" class="p-2 border border-gray-600 rounded-md text-md focus:border-blue-500 focus:ring-blue-500 bg-gray-700 text-gray-400">Mute</button>
          <button id="system-audio" class="p-2 border border-gray-600 rounded-md text-md focus:border-blue-500 focus:ring-blue-500 bg-gray-700 text-gray-400">Send System Audio</button>
        </div>
      </div>
      <div id="entries" style="height:100%; overflow: scroll"></div>
    </div>
    <script type="module">
      import("./renkon.js").then((mod) => mod.view());
    </script>
  </body>
</html>
